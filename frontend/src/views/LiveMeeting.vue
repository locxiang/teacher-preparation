<template>
  <div class="min-h-screen bg-gray-50 flex flex-col">
    <!-- é”™è¯¯å¼¹çª— -->
    <ConfirmDialog
      :visible="showErrorDialog"
      title="æ— æ³•å¼€å§‹ä¼šè®®"
      subtitle="ç¼ºå°‘å¿…è¦çš„ä»»åŠ¡ä¿¡æ¯"
      :message="errorDialogMessage"
      confirm-text="è¿”å›ä¸Šä¼ é¡µé¢"
      cancel-text="è¿”å›é¦–é¡µ"
      :loading="false"
      @confirm="router.push(`/meeting/${meetingId}/upload`)"
      @cancel="router.push('/')"
      @update:visible="showErrorDialog = $event"
    />

    <!-- é¡¶éƒ¨å›ºå®šæ  -->
    <MeetingHeader
      :meeting="meeting"
      :meeting-id="meetingId"
      :message-count="messages.length"
      :task-info="taskInfo"
      :has-microphone-permission="hasMicrophonePermission"
      :is-requesting-permission="isRequestingPermission"
      :is-recording="isRecording"
      @request-permission="requestMicrophonePermission"
      @toggle-recording="handleToggleRecording"
    />

    <!-- ä¸»å†…å®¹åŒºåŸŸ -->
    <div class="flex-1 flex flex-col overflow-hidden">
      <div class="max-w-[1600px] mx-auto px-8 py-4 w-full flex-1 flex flex-col min-h-0">
        <!-- Meeting Stages Indicator -->
        <MeetingStages
          :stages="stages"
          :current-stage-index="currentStageIndex"
        />

        <div class="flex-1 flex space-x-4 overflow-hidden min-h-0">
          <!-- Left Sidebar -->
          <div class="w-64 flex flex-col space-y-4 overflow-y-auto shrink-0">
            <!-- Audio Visualizer -->
            <AudioVisualizer
              :audio-bars="audioBars"
              :current-speaker="currentSpeaker"
              :is-recording="isRecording"
              :silence-duration="silenceDuration"
              :is-a-i-speaking="isAISpeaking"
              :is-a-i-generating="isAIGenerating"
            />

            <!-- Digital Human Video Area -->
            <DigitalHuman />

            <!-- Participants -->
            <ParticipantsList
              :teachers="meeting?.teachers"
            />

            <!-- Reference Docs -->
            <ReferenceDocs :meeting-id="meetingId" />
          </div>

          <!-- Right Column: Chat History -->
          <div class="flex-1 flex flex-col overflow-hidden min-h-0">
            <ChatHistory
              :messages="messages"
              :stages="stages"
              :current-stage-index="currentStageIndex"
            />
          </div>
        </div>
      </div>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, onUnmounted, onMounted } from 'vue'
import { useRoute, useRouter } from 'vue-router'
import { AliyunASRDirectService } from '@/services/aliyun-asr-direct'
import { getMeeting, appendMessage, type Meeting, type MessageData } from '@/services/meeting'
import type { RecognitionResult } from '@/services/aliyun-asr'
import ConfirmDialog from '@/components/ConfirmDialog.vue'
import { streamAIChat } from '@/services/ai-chat'
import { AliyunTTSService, getTTSToken } from '@/services/aliyun-tts'
import MeetingHeader from './liveMeeting/MeetingHeader.vue'
import MeetingStages from './liveMeeting/MeetingStages.vue'
import AudioVisualizer from './liveMeeting/AudioVisualizer.vue'
import ParticipantsList from './liveMeeting/ParticipantsList.vue'
import ReferenceDocs from './liveMeeting/ReferenceDocs.vue'
import DigitalHuman from './liveMeeting/DigitalHuman.vue'
import ChatHistory from './liveMeeting/ChatHistory.vue'

interface Stage {
  id: string
  name: string
  description: string
}

interface Message {
  id: string
  type: 'human' | 'ai'
  speaker: string
  content: string
  timestamp: number // ç»å¯¹æ—¶é—´æˆ³ï¼ˆçœŸå®æ—¶é—´ï¼‰
  relativeTime?: number // ç›¸å¯¹æ—¶é—´ï¼ˆä»å½•éŸ³å¼€å§‹çš„æ¯«ç§’æ•°ï¼‰
  stageIndex: number
  isFinal?: boolean
}

const stages: Stage[] = [
  { id: 'host', name: 'ä¸»æŒäººå‘è¨€', description: 'ä¸»æŒäººä»‹ç»ä¼šè®®ä¸»é¢˜ã€ç›®æ ‡å’Œæµç¨‹ï¼Œæ˜ç¡®æœ¬æ¬¡å¤‡è¯¾çš„é‡ç‚¹ã€‚' },
  { id: 'teachers', name: 'æ•™å¸ˆå‘è¨€', description: 'å„ä½è€å¸ˆè½®æµå‘è¡¨è§‚ç‚¹ï¼ŒAI å®æ—¶è®°å½•å¹¶æ ‡è®°é‡ç‚¹ã€‚' },
  { id: 'discussion', name: 'é›†ä½“ç ”è®¨', description: 'é’ˆå¯¹åˆ†æ­§ç‚¹è¿›è¡Œæ·±å…¥è®¨è®ºï¼ŒAI è¾…åŠ©æ¢³ç†å…±è¯†ã€‚' },
  { id: 'conclusion', name: 'å½¢æˆç»“è®º', description: 'æ€»ç»“ä¼šè®®æˆæœï¼Œç¡®å®šæœ€ç»ˆæ•™å­¦æ–¹æ¡ˆå’Œè¡ŒåŠ¨è®¡åˆ’ã€‚' },
]

const currentStageIndex = ref(0)
const route = useRoute()
const router = useRouter()
const meetingId = ref(route.params.id as string || 'demo-meeting-' + Date.now())
const meeting = ref<Meeting | null>(null)

// è¯­éŸ³è¯†åˆ«æœåŠ¡
let asrService: AliyunASRDirectService | null = null
const isRecording = ref(false)
const currentSpeaker = ref<string | null>(null)
const wsConnected = ref(false)

// éº¦å…‹é£æƒé™ç®¡ç†
const hasMicrophonePermission = ref(false)
const isRequestingPermission = ref(false)
let audioStream: MediaStream | null = null
const errorMessage = ref<string>('')

// ä»»åŠ¡ç®¡ç†
const taskInfo = ref<{ TaskId?: string; MeetingJoinUrl?: string; TaskStatus?: string } | null>(null)

// æ¶ˆæ¯åˆ—è¡¨
const messages = ref<Message[]>([])
const currentTranscriptMessageId = ref<string | null>(null)
const savedMessageIds = ref<Set<string>>(new Set()) // å·²ä¿å­˜çš„æ¶ˆæ¯IDé›†åˆï¼Œé¿å…é‡å¤ä¿å­˜

// éŸ³é¢‘å¯è§†åŒ–æ•°æ®
const audioBars = ref<number[]>(Array(50).fill(2))

// é™é»˜è®¡æ—¶å™¨ç›¸å…³
const lastSpeechTimestamp = ref<number | null>(null)
const silenceDuration = ref<number>(0)
let silenceTimer: number | null = null

// éŸ³é¢‘éŸ³é‡æ£€æµ‹ç›¸å…³
const SILENCE_DELAY = 500
const BASELINE_SAMPLES = 100
const VARIANCE_WINDOW = 20
const VARIANCE_THRESHOLD = 0.5
let consecutiveSpeechFrames = 0
let isSpeaking = false
let lastSpeechDetectedTime = 0
let silenceStartTime: number | null = null
let volumeDebugCount = 0
let recordingStartTime = 0

// åŠ¨æ€é˜ˆå€¼ç›¸å…³
let volumeHistory: number[] = []
let volumeWindow: number[] = []
let noiseBaseline = 0
let baselineCalculated = false
let samplesCollected = 0

// AIè¯´è¯ç›¸å…³
const isAISpeaking = ref(false)
const isAIGenerating = ref(false)
let ttsService: AliyunTTSService | null = null
let pendingTextBuffer = ''
const SILENCE_THRESHOLD = 10
let aiSpeechStartTime: number | null = null // AIå¼€å§‹è¯´è¯çš„æ—¶é—´æˆ³ï¼Œç”¨äºæš‚åœé™é»˜è®¡æ—¶

// å¤„ç†è¯­éŸ³è¯†åˆ«ç»“æœ
const handleRecognitionResult = (result: RecognitionResult) => {
  const speaker = result.speaker || 'æœªçŸ¥'

  // å½“è¯†åˆ«åˆ°æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹æ—¶ï¼Œæ‰æ‰“æ–­ AI è¯´è¯ï¼ˆé¿å…è¢«æ— æ„ä¹‰çš„å™ªéŸ³æ‰“æ–­ï¼‰
  if (result.text && result.text.trim() && (isAISpeaking.value || isAIGenerating.value)) {
    console.log('[LiveMeeting] ğŸ›‘ è¯†åˆ«åˆ°æ–°å†…å®¹ï¼Œåœæ­¢AIè¯´è¯:', result.text.substring(0, 20))
    stopAIVoice()
    isAIGenerating.value = false
  }

  if (result.isFinal) {
    if (currentTranscriptMessageId.value) {
      const index = messages.value.findIndex(m => m.id === currentTranscriptMessageId.value)
      if (index !== -1) {
        messages.value[index].content = result.text
        messages.value[index].isFinal = true
      }
      currentTranscriptMessageId.value = null
    } else {
      const newMessage: Message = {
        id: `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        type: 'human',
        speaker: speaker,
        content: result.text,
        timestamp: result.timestamp,
        relativeTime: result.relativeTime,
        stageIndex: currentStageIndex.value,
        isFinal: true,
      }
      messages.value.push(newMessage)
    }
  } else {
    if (!currentTranscriptMessageId.value) {
      const tempMessage: Message = {
        id: `temp_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        type: 'human',
        speaker: speaker,
        content: result.text,
        timestamp: result.timestamp,
        relativeTime: result.relativeTime,
        stageIndex: currentStageIndex.value,
        isFinal: false,
      }
      messages.value.push(tempMessage)
      currentTranscriptMessageId.value = tempMessage.id
    } else {
      const index = messages.value.findIndex(m => m.id === currentTranscriptMessageId.value)
      if (index !== -1) {
        messages.value[index].content = result.text
        messages.value[index].speaker = speaker
        messages.value[index].timestamp = result.timestamp
        messages.value[index].relativeTime = result.relativeTime
      }
    }
  }

  if (speaker && speaker !== 'æœªçŸ¥') {
    currentSpeaker.value = speaker
  }

  if (result.isFinal) {
    // æ‰¾åˆ°å¯¹åº”çš„æ¶ˆæ¯å¹¶ä¿å­˜
    let messageToSave: Message | null = null

    if (currentTranscriptMessageId.value) {
      // æ›´æ–°ç°æœ‰æ¶ˆæ¯
      const index = messages.value.findIndex(m => m.id === currentTranscriptMessageId.value)
      if (index !== -1) {
        messageToSave = messages.value[index]
      }
      currentTranscriptMessageId.value = null
    } else {
      // æ–°åˆ›å»ºçš„æ¶ˆæ¯ï¼ˆåœ¨ else åˆ†æ”¯ä¸­åˆ›å»ºçš„ï¼‰
      const index = messages.value.length - 1
      if (index >= 0) {
        messageToSave = messages.value[index]
      }
    }

    if (messageToSave && messageToSave.isFinal) {
      saveMessageToDatabase(messageToSave)
    }
  }
}

// ä¿å­˜å•æ¡æ¶ˆæ¯åˆ°æ•°æ®åº“ï¼ˆå¢é‡ä¿å­˜ï¼‰
const saveMessageToDatabase = async (message: Message) => {
  // æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²ä¿å­˜
  if (savedMessageIds.value.has(message.id)) {
    return
  }

  // åªä¿å­˜æœ€ç»ˆç»“æœçš„æ¶ˆæ¯
  if (!message.isFinal) {
    return
  }

  // å¿…é¡»æœ‰å†…å®¹
  if (!message.content || !message.content.trim()) {
    return
  }

  // å¯¹äºAIæ¶ˆæ¯ï¼Œé¢å¤–æ£€æŸ¥å†…å®¹æ˜¯å¦å®Œæ•´
  if (message.type === 'ai') {
    const content = message.content.trim()
    if (content === '' || content === 'AIå›ç­”ç”Ÿæˆå¤±è´¥') {
      return
    }
  }

  try {
    const messageData: MessageData = {
      name: message.speaker || 'æœªçŸ¥',
      time: message.timestamp,
      type: message.type,
      content: message.content.trim(),
    }

    await appendMessage(meetingId.value, messageData)
    savedMessageIds.value.add(message.id)
    console.log(`[LiveMeeting] âœ… æ¶ˆæ¯å·²ä¿å­˜: ${message.type === 'ai' ? 'AI' : 'äººç±»'} - ${message.speaker}`)
  } catch (error) {
    console.error('[LiveMeeting] âŒ ä¿å­˜æ¶ˆæ¯å¤±è´¥:', error)
  }
}

// ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“ï¼ˆæ‰¹é‡ä¿å­˜ï¼Œç”¨äºå…¼å®¹æ—§é€»è¾‘ï¼‰
const saveMessagesToDatabase = async () => {
  try {
    // æ‰¾å‡ºæ‰€æœ‰æœªä¿å­˜çš„æœ€ç»ˆæ¶ˆæ¯
    const unsavedMessages = messages.value.filter(msg => {
      if (savedMessageIds.value.has(msg.id)) {
        return false
      }
      if (!msg.isFinal) {
        return false
      }
      if (!msg.content || !msg.content.trim()) {
        return false
      }
      if (msg.type === 'ai') {
        const content = msg.content.trim()
        if (content === '' || content === 'AIå›ç­”ç”Ÿæˆå¤±è´¥') {
          return false
        }
      }
      return true
    })

    if (unsavedMessages.length === 0) {
      return
    }

    // æŒ‰æ—¶é—´æˆ³æ’åºï¼Œé€ä¸ªä¿å­˜
    const sortedMessages = [...unsavedMessages].sort((a, b) => a.timestamp - b.timestamp)

    for (const msg of sortedMessages) {
      await saveMessageToDatabase(msg)
    }

    console.log(`[LiveMeeting] âœ… æ‰¹é‡ä¿å­˜å®Œæˆï¼ˆå…± ${sortedMessages.length} æ¡æ¶ˆæ¯ï¼‰`)
  } catch (error) {
    console.error('[LiveMeeting] âŒ ä¿å­˜èŠå¤©è®°å½•å¤±è´¥:', error)
  }
}

// å¤„ç†è¯†åˆ«é”™è¯¯
const handleRecognitionError = (error: Error) => {
  console.error('Recognition error:', error)
  errorMessage.value = error.message || 'è¯­éŸ³è¯†åˆ«é”™è¯¯'
}

// è®¡ç®—éŸ³é¢‘æ•°æ®çš„RMSï¼ˆå‡æ–¹æ ¹ï¼‰éŸ³é‡
const calculateRMSVolume = (data: Uint8Array): number => {
  if (data.length === 0) {
    return 0
  }

  let sum = 0
  for (let i = 0; i < data.length; i++) {
    const sample = (data[i] - 128) / 128
    sum += sample * sample
  }

  const rms = Math.sqrt(sum / data.length)
  const percentage = Math.min(100, Math.max(0, rms * 1000))
  return percentage
}

// è®¡ç®—éŸ³é‡æ•°ç»„çš„æ–¹å·®
const calculateVariance = (values: number[]): number => {
  if (values.length < 2) {
    return 0
  }

  const mean = values.reduce((sum, val) => sum + val, 0) / values.length
  const variance = values.reduce((sum, val) => {
    const diff = val - mean
    return sum + diff * diff
  }, 0) / values.length

  return variance
}

// å¤„ç†éŸ³é¢‘æ•°æ®
const handleAudioData = (data: Uint8Array) => {
  if (!isRecording.value) {
    audioBars.value = Array(50).fill(2)
    return
  }

  const barCount = audioBars.value.length
  const dataLength = data.length

  if (dataLength === 0) {
    audioBars.value = Array(50).fill(2)
    return
  }

  // è®¡ç®—éŸ³é¢‘å¯è§†åŒ–æ•°æ®
  const step = Math.floor(dataLength / barCount)
  const newBars: number[] = []

  for (let i = 0; i < barCount; i++) {
    const index = i * step
    if (index < dataLength) {
      const value = data[index] || 0
      const percentage = Math.max(2, Math.min(100, (value / 255) * 100))
      newBars.push(percentage)
    } else {
      newBars.push(2)
    }
  }

  audioBars.value = newBars

  // æ£€æµ‹æ˜¯å¦æœ‰æœ‰æ•ˆè¯­éŸ³
  const currentVolume = calculateRMSVolume(data)
  const now = Date.now()

  // æ”¶é›†æ ·æœ¬è®¡ç®—ç¯å¢ƒå™ªéŸ³åŸºçº¿
  if (!baselineCalculated) {
    volumeHistory.push(currentVolume)
    samplesCollected++

    if (samplesCollected >= BASELINE_SAMPLES) {
      const sorted = [...volumeHistory].sort((a, b) => a - b)
      const medianIndex = Math.floor(sorted.length / 2)
      noiseBaseline = sorted[medianIndex]
      baselineCalculated = true
      console.log(`[éŸ³é‡æ£€æµ‹] ç¯å¢ƒå™ªéŸ³åŸºçº¿å·²è®¡ç®—: ${noiseBaseline.toFixed(2)}%`)

      if (!isSpeaking && silenceStartTime === null) {
        silenceStartTime = recordingStartTime
        lastSpeechTimestamp.value = recordingStartTime
      }
    }
  }

  // ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—éŸ³é‡æ–¹å·®
  volumeWindow.push(currentVolume)
  if (volumeWindow.length > VARIANCE_WINDOW) {
    volumeWindow.shift()
  }

  const variance = volumeWindow.length >= 2 ? calculateVariance(volumeWindow) : 0

  volumeDebugCount++
  if (volumeDebugCount % 50 === 0) {
    console.log(`[éŸ³é‡æ£€æµ‹] å½“å‰éŸ³é‡: ${currentVolume.toFixed(2)}%, åŸºçº¿: ${noiseBaseline.toFixed(2)}%, æ–¹å·®: ${variance.toFixed(4)}, é˜ˆå€¼: ${VARIANCE_THRESHOLD}, æ˜¯å¦è¯´è¯: ${isSpeaking}, é™é»˜æ—¶é•¿: ${silenceDuration.value}ç§’`)
  }

  // åŸºäºæ–¹å·®åˆ¤æ–­æ˜¯å¦æœ‰äººè¯´è¯
  if (variance > VARIANCE_THRESHOLD) {
    consecutiveSpeechFrames++
    lastSpeechDetectedTime = now

    if (consecutiveSpeechFrames >= 2) {
      if (!isSpeaking) {
        isSpeaking = true
        lastSpeechTimestamp.value = now
        silenceStartTime = null
        silenceDuration.value = 0

        // æ³¨æ„ï¼šä¸å†åŸºäºéŸ³é‡æ£€æµ‹æ‰“æ–­ AIï¼Œæ”¹ä¸ºåœ¨è¯­éŸ³è¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹æ—¶æ‰“æ–­ï¼ˆè§ handleRecognitionResultï¼‰
      } else {
        lastSpeechTimestamp.value = now
      }
    }
  } else {
    consecutiveSpeechFrames = 0

    if (isSpeaking && lastSpeechDetectedTime > 0) {
      const timeSinceLastSpeech = now - lastSpeechDetectedTime
      if (timeSinceLastSpeech >= SILENCE_DELAY) {
        isSpeaking = false
        silenceStartTime = lastSpeechTimestamp.value || lastSpeechDetectedTime
        console.log(`[éŸ³é‡æ£€æµ‹] è¿›å…¥é™é»˜çŠ¶æ€ï¼Œé™é»˜å¼€å§‹æ—¶é—´: ${new Date(silenceStartTime).toLocaleTimeString()}`)
      }
    } else if (!isSpeaking && baselineCalculated && silenceStartTime === null) {
      silenceStartTime = recordingStartTime
      lastSpeechTimestamp.value = recordingStartTime
    }
  }
}

// åˆå§‹åŒ–è¯­éŸ³åˆæˆ
const initTTS = async (): Promise<void> => {
  console.log('[LiveMeeting] å¼€å§‹åˆå§‹åŒ–è¯­éŸ³åˆæˆæœåŠ¡')
  try {
    const { token, app_key } = await getTTSToken()
    ttsService = new AliyunTTSService()

    await ttsService.startSynthesis(
      {
        token,
        appKey: app_key,
        voice: 'longxiaochun',
        format: 'PCM',
        sampleRate: 24000,
        volume: 100,
        speechRate: 0,
        pitchRate: 0,
      },
      () => {
        console.log('[LiveMeeting] âœ… è¯­éŸ³åˆæˆå®Œæˆ')
      },
      (error: Error) => {
        console.error('[LiveMeeting] âŒ è¯­éŸ³åˆæˆé”™è¯¯:', error)
        isAISpeaking.value = false
        errorMessage.value = `è¯­éŸ³åˆæˆå¤±è´¥: ${error.message}`
      },
      () => {
        console.log('[LiveMeeting] âœ… éŸ³é¢‘æ’­æ”¾å®Œæˆ')
        isAISpeaking.value = false

        // AIæ’­æ”¾å®Œæˆï¼Œé‡ç½®é™é»˜å¼€å§‹æ—¶é—´ï¼Œé‡æ–°å¼€å§‹é™é»˜è®¡æ—¶
        if (aiSpeechStartTime !== null && isRecording.value) {
          const now = Date.now()
          // é‡ç½®é™é»˜å¼€å§‹æ—¶é—´ä¸ºå½“å‰æ—¶é—´ï¼Œé‡æ–°å¼€å§‹è®¡æ—¶
          silenceStartTime = now
          lastSpeechTimestamp.value = now
          silenceDuration.value = 0
          aiSpeechStartTime = null
          console.log('[LiveMeeting] âœ… AIæ’­æ”¾å®Œæˆï¼Œé‡æ–°å¼€å§‹é™é»˜è®¡æ—¶')
        }
      },
    )
  } catch (error) {
    console.error('[LiveMeeting] âŒ åˆå§‹åŒ–è¯­éŸ³åˆæˆå¤±è´¥:', error)
    isAISpeaking.value = false
    errorMessage.value = `è¯­éŸ³åˆæˆåˆå§‹åŒ–å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
  }
}

// å‘é€æ–‡æœ¬è¿›è¡Œè¯­éŸ³åˆæˆ
const synthesizeText = (text: string) => {
  if (!ttsService) {
    return
  }

  try {
    pendingTextBuffer += text

    if (/[ã€‚ï¼ï¼Ÿ\n]/.test(text)) {
      const sentences = pendingTextBuffer.split(/([ã€‚ï¼ï¼Ÿ\n])/)
      for (let i = 0; i < sentences.length - 1; i += 2) {
        const sentence = sentences[i] + sentences[i + 1]
        if (sentence.trim()) {
          ttsService.sendText(sentence.trim())
        }
      }
      pendingTextBuffer = sentences[sentences.length - 1] || ''
    } else if (pendingTextBuffer.length >= 20) {
      ttsService.sendText(pendingTextBuffer)
      pendingTextBuffer = ''
    }
  } catch (error) {
    console.error('[LiveMeeting] âŒ å‘é€æ–‡æœ¬åˆ°TTSå¤±è´¥:', error)
  }
}

// åœæ­¢AIè¯­éŸ³æ’­æ”¾
const stopAIVoice = () => {
  if (ttsService) {
    ttsService.stopPlayback()
    ttsService.stopSynthesis()
    ttsService.close()
    ttsService = null
  }
  isAISpeaking.value = false
  pendingTextBuffer = ''

  // å¦‚æœAIè¢«ä¸­æ–­ï¼Œé‡ç½®é™é»˜å¼€å§‹æ—¶é—´
  if (aiSpeechStartTime !== null && isRecording.value) {
    const now = Date.now()
    silenceStartTime = now
    lastSpeechTimestamp.value = now
    silenceDuration.value = 0
    aiSpeechStartTime = null
    console.log('[LiveMeeting] âœ… AIè¢«ä¸­æ–­ï¼Œé‡æ–°å¼€å§‹é™é»˜è®¡æ—¶')
  }
}

// è§¦å‘AIè¯´è¯
const triggerAISpeech = async () => {
  if (isAISpeaking.value || isAIGenerating.value || !isRecording.value) {
    return
  }

  // æ£€æŸ¥æœ€æ–°ä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯AIå‘é€çš„ï¼Œå¦‚æœæ˜¯åˆ™ä¸è§¦å‘
  // ä»æ•°ç»„æœ«å°¾å¼€å§‹æŸ¥æ‰¾æœ€æ–°çš„æœ€ç»ˆæ¶ˆæ¯ï¼ˆå› ä¸ºæ–°æ¶ˆæ¯æ€»æ˜¯ push åˆ°æ•°ç»„æœ«å°¾ï¼‰
  let latestMessage: Message | null = null
  for (let i = messages.value.length - 1; i >= 0; i--) {
    if (messages.value[i].isFinal) {
      latestMessage = messages.value[i]
      break
    }
  }

  if (latestMessage) {
    console.log(`[LiveMeeting] ğŸ” æœ€æ–°æœ€ç»ˆæ¶ˆæ¯: ${latestMessage.type === 'ai' ? 'AI' : 'äººç±»'} - ${latestMessage.speaker} - ${latestMessage.content.substring(0, 20)}`)
  }

  if (latestMessage && latestMessage.type === 'ai') {
    console.log('[LiveMeeting] â¸ï¸ æœ€æ–°ä¸€æ¡æ¶ˆæ¯æ˜¯AIå‘é€çš„ï¼Œè·³è¿‡è§¦å‘AIè¯´è¯')
    return
  }

  console.log('[LiveMeeting] ğŸ¯ è§¦å‘AIè¯´è¯ï¼Œé™é»˜æ—¶é•¿:', silenceDuration.value, 'ç§’')

  // è®°å½•AIå¼€å§‹è¯´è¯çš„æ—¶é—´ï¼Œæš‚åœé™é»˜è®¡æ—¶
  aiSpeechStartTime = Date.now()
  isAIGenerating.value = true
  isAISpeaking.value = true
  pendingTextBuffer = ''

  // æš‚åœé™é»˜è®¡æ—¶ï¼ˆä¸æ¸…é™¤ silenceStartTimeï¼Œä½†ä¼šåœ¨è®¡æ—¶å™¨ä¸­æ£€æŸ¥ AI çŠ¶æ€ï¼‰
  console.log('[LiveMeeting] â¸ï¸ AIå¼€å§‹è¯´è¯ï¼Œæš‚åœé™é»˜è®¡æ—¶')

  const humanMessages = messages.value
    .filter(msg => msg.type === 'human' && msg.isFinal)
    .slice(-10)

  let chatHistoryText = ''
  if (humanMessages.length > 0) {
    chatHistoryText = humanMessages
      .map(msg => {
        const speaker = msg.speaker || 'æœªçŸ¥'
        return `${speaker}: ${msg.content}`
      })
      .join('\n')
  } else {
    chatHistoryText = `ä¼šè®®ä¸»é¢˜: ${meeting.value?.name || 'å¤‡è¯¾ä¼šè®®'}`
  }

  await initTTS()

  const aiMessageId = `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
  let aiResponseText = ''

  try {
    await streamAIChat(
      meetingId.value || undefined,
      chatHistoryText,
      (chunk: string) => {
        aiResponseText += chunk

        const existingIndex = messages.value.findIndex(m => m.id === aiMessageId)
        if (existingIndex === -1) {
          const newMessage: Message = {
            id: aiMessageId,
            type: 'ai',
            speaker: 'AIåŠ©æ‰‹',
            content: aiResponseText,
            timestamp: Date.now(),
            relativeTime: undefined, // AIæ¶ˆæ¯æ²¡æœ‰ç›¸å¯¹æ—¶é—´
            stageIndex: currentStageIndex.value,
            isFinal: false,
          }
          messages.value.push(newMessage)
        } else {
          messages.value[existingIndex].content = aiResponseText
        }

        if (ttsService) {
          synthesizeText(chunk)
        }
      },
      () => {
        isAIGenerating.value = false

        const existingIndex = messages.value.findIndex(m => m.id === aiMessageId)
        if (existingIndex !== -1) {
          messages.value[existingIndex].isFinal = true
          // AIæ¶ˆæ¯å®Œæˆï¼Œç«‹å³ä¿å­˜å•æ¡æ¶ˆæ¯
          saveMessageToDatabase(messages.value[existingIndex])
        }

        if (ttsService) {
          if (pendingTextBuffer.trim()) {
            ttsService.sendText(pendingTextBuffer.trim())
            pendingTextBuffer = ''
          }
          ttsService.stopSynthesis()
        }
      },
      (error: Error) => {
        console.error('[LiveMeeting] âŒ AIå¯¹è¯å¤±è´¥:', error)
        errorMessage.value = error.message || 'AIå¯¹è¯å¤±è´¥ï¼Œè¯·é‡è¯•'
        isAIGenerating.value = false
        isAISpeaking.value = false
        stopAIVoice()

        const existingIndex = messages.value.findIndex(m => m.id === aiMessageId)
        if (existingIndex !== -1) {
          messages.value[existingIndex].isFinal = true
          messages.value[existingIndex].content = aiResponseText || 'AIå›ç­”ç”Ÿæˆå¤±è´¥'
          // å³ä½¿å‡ºé”™ï¼Œä¹Ÿä¿å­˜å·²ç”Ÿæˆçš„æ¶ˆæ¯
          saveMessageToDatabase(messages.value[existingIndex])
        }
      },
    )
  } catch (error) {
    console.error('[LiveMeeting] âŒ AIå¯¹è¯å¼‚å¸¸:', error)
    errorMessage.value = error instanceof Error ? error.message : 'AIå¯¹è¯å¤±è´¥ï¼Œè¯·é‡è¯•'
    isAIGenerating.value = false
    isAISpeaking.value = false
    stopAIVoice()
  }
}

// è¯·æ±‚éº¦å…‹é£æƒé™
const requestMicrophonePermission = async () => {
  isRequestingPermission.value = true
  errorMessage.value = ''

  try {
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      throw new Error('æµè§ˆå™¨ä¸æ”¯æŒéº¦å…‹é£è®¿é—®ï¼Œè¯·ä½¿ç”¨ HTTPS åè®®æˆ–æ›´æ–°æµè§ˆå™¨')
    }

    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 16000,
        channelCount: 1,
      },
    })

    audioStream = stream
    hasMicrophonePermission.value = true
    console.log('éº¦å…‹é£æˆæƒæˆåŠŸ')
  } catch (error) {
    console.error('Failed to request microphone permission:', error)
    const err = error as DOMException
    errorMessage.value = err.name === 'NotAllowedError'
      ? 'æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æµè§ˆå™¨æƒé™è®¾ç½®'
      : err.name === 'NotFoundError'
      ? 'æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡'
      : err.message || 'æ— æ³•è·å–éº¦å…‹é£æƒé™ï¼Œè¯·æ£€æŸ¥è®¾å¤‡è¿æ¥'

    hasMicrophonePermission.value = false
    if (audioStream) {
      audioStream.getTracks().forEach(track => track.stop())
      audioStream = null
    }
  } finally {
    isRequestingPermission.value = false
  }
}

// åˆ‡æ¢å½•éŸ³çŠ¶æ€
const handleToggleRecording = () => {
  if (isRecording.value) {
    handleStopRecording()
  } else {
    handleStartRecording()
  }
}

// å¼€å§‹å½•éŸ³
const handleStartRecording = async () => {
  try {
    errorMessage.value = ''

    // æ£€æŸ¥éº¦å…‹é£æƒé™å’ŒéŸ³é¢‘æµçŠ¶æ€
    if (!hasMicrophonePermission.value || !audioStream) {
      errorMessage.value = 'è¯·å…ˆæˆæƒéº¦å…‹é£'
      return
    }

    // æ£€æŸ¥éŸ³é¢‘æµçš„ tracks æ˜¯å¦ä»ç„¶æ´»è·ƒ
    const activeTracks = audioStream.getTracks().filter(track => track.readyState === 'live')
    if (activeTracks.length === 0) {
      console.log('[LiveMeeting] éŸ³é¢‘æµ tracks å·²åœæ­¢ï¼Œé‡æ–°è·å–éº¦å…‹é£æƒé™')
      // æ¸…ç†æ—§çš„æµ
      audioStream.getTracks().forEach(track => track.stop())
      audioStream = null
      hasMicrophonePermission.value = false

      // é‡æ–°è·å–éº¦å…‹é£æƒé™
      await requestMicrophonePermission()

      if (!hasMicrophonePermission.value || !audioStream) {
        errorMessage.value = 'æ— æ³•é‡æ–°è·å–éº¦å…‹é£æƒé™'
        return
      }
    }

    if (!taskInfo.value?.MeetingJoinUrl) {
      errorMessage.value = 'è¯·å…ˆåˆ›å»ºå®æ—¶è®°å½•'
      return
    }

    const wsUrl = taskInfo.value.MeetingJoinUrl
    asrService = new AliyunASRDirectService()

    asrService.onResult(handleRecognitionResult)
    asrService.onError(handleRecognitionError)
    asrService.onAudioData(handleAudioData)

    await asrService.startRecognition(wsUrl, audioStream)

    isRecording.value = true
    wsConnected.value = true
    startSilenceTimer()
  } catch (error) {
    console.error('Failed to start recording:', error)
    const err = error instanceof Error ? error : new Error('æ— æ³•å¼€å§‹å½•éŸ³')
    errorMessage.value = err.message || 'æ— æ³•å¼€å§‹å½•éŸ³ï¼Œè¯·æ£€æŸ¥è®¾å¤‡è¿æ¥'
    isRecording.value = false
    wsConnected.value = false
    asrService = null
  }
}

// åœæ­¢å½•éŸ³
const handleStopRecording = async () => {
  if (asrService) {
    asrService.stopRecognition()
    asrService = null
  }

  isRecording.value = false
  wsConnected.value = false
  stopSilenceTimer()
  stopAIVoice()
  isAIGenerating.value = false

  if (currentTranscriptMessageId.value) {
    const index = messages.value.findIndex(m => m.id === currentTranscriptMessageId.value)
    if (index !== -1 && !messages.value[index].isFinal) {
      messages.value.splice(index, 1)
    }
    currentTranscriptMessageId.value = null
  }

  await saveMessagesToDatabase()
  audioBars.value = Array(50).fill(2)
}

// é‡Šæ”¾éº¦å…‹é£æƒé™
const releaseMicrophone = () => {
  handleStopRecording()

  if (audioStream) {
    audioStream.getTracks().forEach(track => track.stop())
    audioStream = null
  }

  hasMicrophonePermission.value = false
}

// å¯åŠ¨é™é»˜è®¡æ—¶å™¨
const startSilenceTimer = () => {
  if (silenceTimer) {
    return
  }

  recordingStartTime = Date.now()
  lastSpeechTimestamp.value = null
  silenceDuration.value = 0
  consecutiveSpeechFrames = 0
  isSpeaking = false
  lastSpeechDetectedTime = 0
  silenceStartTime = null

  volumeHistory = []
  volumeWindow = []
  noiseBaseline = 0
  baselineCalculated = false
  samplesCollected = 0

  silenceTimer = window.setInterval(() => {
    if (!isRecording.value) {
      return
    }

    // å¦‚æœAIæ­£åœ¨è¯´è¯æˆ–ç”Ÿæˆä¸­ï¼Œæš‚åœé™é»˜è®¡æ—¶
    if (isAISpeaking.value || isAIGenerating.value) {
      // ä¿æŒå½“å‰çš„ silenceDuration ä¸å˜ï¼Œä¸å¢åŠ è®¡æ—¶
      // ä½†ä¹Ÿä¸é‡ç½®ä¸º0ï¼Œä¿æŒå½“å‰å€¼
      return
    }

    if (!isSpeaking) {
      if (silenceStartTime !== null) {
        const now = Date.now()
        const elapsed = Math.floor((now - silenceStartTime) / 1000)
        silenceDuration.value = elapsed

        if (elapsed >= SILENCE_THRESHOLD && !isAISpeaking.value && !isAIGenerating.value) {
          console.log(`[LiveMeeting] â° é™é»˜æ—¶é•¿è¾¾åˆ°${SILENCE_THRESHOLD}ç§’ï¼Œè§¦å‘AIè¯´è¯`)
          triggerAISpeech()
        }
      } else if (baselineCalculated && recordingStartTime > 0) {
        const now = Date.now()
        const elapsed = Math.floor((now - recordingStartTime) / 1000)
        silenceDuration.value = elapsed

        if (elapsed >= SILENCE_THRESHOLD && !isAISpeaking.value && !isAIGenerating.value) {
          console.log(`[LiveMeeting] â° é™é»˜æ—¶é•¿è¾¾åˆ°${SILENCE_THRESHOLD}ç§’ï¼Œè§¦å‘AIè¯´è¯`)
          triggerAISpeech()
        }
      } else {
        silenceDuration.value = 0
      }
    } else {
      silenceDuration.value = 0
    }
  }, 1000)
}

// åœæ­¢é™é»˜è®¡æ—¶å™¨
const stopSilenceTimer = () => {
  if (silenceTimer) {
    clearInterval(silenceTimer)
    silenceTimer = null
  }
  silenceDuration.value = 0
  lastSpeechTimestamp.value = null
  consecutiveSpeechFrames = 0
  isSpeaking = false
  lastSpeechDetectedTime = 0
  silenceStartTime = null
  aiSpeechStartTime = null

  volumeHistory = []
  volumeWindow = []
  noiseBaseline = 0
  baselineCalculated = false
  samplesCollected = 0
}

// é”™è¯¯å¼¹çª—
const showErrorDialog = ref(false)
const errorDialogMessage = ref('')

// è§£æè½¬å†™æ–‡æœ¬ä¸ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆæ”¯æŒ JSONL æ ¼å¼å’Œæ—§æ ¼å¼ï¼‰
const parseTranscriptToMessages = (transcriptText: string): Message[] => {
  if (!transcriptText || !transcriptText.trim()) {
    return []
  }

  const lines = transcriptText.split('\n').filter(line => line.trim())
  const parsedMessages: Message[] = []

  lines.forEach((line, index) => {
    try {
      // å°è¯•è§£æ JSONL æ ¼å¼ï¼ˆæ–°æ ¼å¼ï¼‰
      const messageData = JSON.parse(line)
      if (messageData.name && messageData.time && messageData.type && messageData.content) {
        parsedMessages.push({
          id: `history_${index}_${messageData.time}`,
          type: messageData.type === 'ai' ? 'ai' : 'human',
          speaker: messageData.name,
          content: messageData.content,
          timestamp: messageData.time,
          stageIndex: currentStageIndex.value,
          isFinal: true,
        })
        // æ ‡è®°ä¸ºå·²ä¿å­˜
        savedMessageIds.value.add(`history_${index}_${messageData.time}`)
        return
      }
    } catch {
      // ä¸æ˜¯ JSON æ ¼å¼ï¼Œå°è¯•è§£ææ—§æ ¼å¼ï¼ˆè¯´è¯äºº: å†…å®¹ï¼‰
    }

    // å°è¯•è§£ææ—§æ ¼å¼ï¼ˆè¯´è¯äºº: å†…å®¹ï¼‰
    const match = line.match(/^(.+?)[ï¼š:]\s*(.+)$/)
    if (match) {
      const speakerName = match[1].trim()
      const content = match[2].trim()

      // åˆ¤æ–­æ˜¯å¦æ˜¯AIæ¶ˆæ¯
      const isAI = speakerName.includes('AI') || speakerName.includes('åŠ©æ‰‹')
      const meetingStartTime = meeting.value?.created_at ? new Date(meeting.value.created_at).getTime() : Date.now()

      parsedMessages.push({
        id: `history_${index}_${Date.now()}`,
        type: isAI ? 'ai' : 'human',
        speaker: speakerName,
        content: content,
        timestamp: meetingStartTime + index * 1000,
        stageIndex: currentStageIndex.value,
        isFinal: true,
      })
    } else if (line.trim()) {
      // å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ¼å¼ï¼Œä½œä¸ºæ™®é€šæ¶ˆæ¯
      const meetingStartTime = meeting.value?.created_at ? new Date(meeting.value.created_at).getTime() : Date.now()
      parsedMessages.push({
        id: `history_${index}_${Date.now()}`,
        type: 'human',
        speaker: 'æœªçŸ¥',
        content: line.trim(),
        timestamp: meetingStartTime + index * 1000,
        stageIndex: currentStageIndex.value,
        isFinal: true,
      })
    }
  })

  return parsedMessages
}

// åŠ è½½ä¼šè®®ä¿¡æ¯
const loadMeeting = async () => {
  try {
    const data = await getMeeting(meetingId.value)
    meeting.value = data

    // åŠ è½½å†å²èŠå¤©è®°å½•
    if (data.transcript) {
      const historyMessages = parseTranscriptToMessages(data.transcript)
      if (historyMessages.length > 0) {
        // å°†å†å²æ¶ˆæ¯æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ï¼ˆå†å²æ¶ˆæ¯åœ¨åº•éƒ¨ï¼Œæ–°æ¶ˆæ¯ä¼šè¿½åŠ åœ¨åé¢ï¼‰
        messages.value = historyMessages
        console.log(`[LiveMeeting] åŠ è½½äº† ${historyMessages.length} æ¡å†å²æ¶ˆæ¯`)
      }
    } else if (data.transcripts && data.transcripts.length > 0) {
      // å¦‚æœæœ‰å¤šä¸ªtranscriptï¼Œä½¿ç”¨æœ€æ–°çš„
      const latestTranscript = data.transcripts[data.transcripts.length - 1]
      if (latestTranscript.text) {
        const historyMessages = parseTranscriptToMessages(latestTranscript.text)
        if (historyMessages.length > 0) {
          messages.value = historyMessages
          console.log(`[LiveMeeting] ä»transcriptsåŠ è½½äº† ${historyMessages.length} æ¡å†å²æ¶ˆæ¯`)
        }
      }
    }

    if (data.task_id && data.stream_url) {
      taskInfo.value = {
        TaskId: data.task_id,
        MeetingJoinUrl: data.stream_url,
        TaskStatus: 'NEW',
      }
      console.log('ä½¿ç”¨ä¼šè®®å·²æœ‰çš„ä»»åŠ¡ä¿¡æ¯:', taskInfo.value)
    } else {
      errorDialogMessage.value = 'ä¼šè®®æ²¡æœ‰å…³è”çš„å®æ—¶è®°å½•ä»»åŠ¡ã€‚\n\nè¯·è¿”å›ä¸Šä¼ èµ„æ–™é¡µé¢ï¼Œç‚¹å‡»"å¼€å§‹ä¼šè®®"æŒ‰é’®åˆ›å»ºä»»åŠ¡ã€‚'
      showErrorDialog.value = true
      console.error('ä¼šè®®ç¼ºå°‘ä»»åŠ¡ä¿¡æ¯:', data)
    }
  } catch (error) {
    console.error('Failed to load meeting:', error)
    errorDialogMessage.value = 'åŠ è½½ä¼šè®®ä¿¡æ¯å¤±è´¥'
    showErrorDialog.value = true
  }
}

onUnmounted(() => {
  handleStopRecording()
  releaseMicrophone()
  stopSilenceTimer()
})

onMounted(async () => {
  await loadMeeting()
})
</script>
